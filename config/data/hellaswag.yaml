name: "hellaswag"
batch_size: 1

noise_function: "constant"
candidate_token_ids_path: data/candidate_tokens/candidate_token_ids_gpt2_mrpc.npy  # data/candidate_token_ids_big.npy # data/candidate_token_ids.npy

truncated_seq_len: 32
max_num_samples: null

# If you want to use precomputed noisy embeddings, set this to the path of the embeddings
noisy_embeddings_path: data/input_based_noise # null # "data/open_orca_testing_dataset_small_embeddings_512.npy"
load_noisy_embeddings: false

# Path to the language model
lm_path: "/models/jay/llama3_2/LLama-3.2-1B-Instruct" # "../LLama-3.2-1B-Instruct" # "../LLama-3.2-1B-Instruct"
vocab_size: 128256
embedding_dim: 2048

# Path to the embedding model
embedding_model_path:  "/models/jay/llama3_2/LLama-3.2-1B-Instruct" # ../LLama-3.2-1B-Instruct"  # "gpt2-medium"
translation_dict_path: data/translation/llama_llama_translation.json

# Sample ids to use
# We need this because of the cross-tokenization issue
# If you want to use all the samples, set this to null
sample_ids_to_use_path: "data/dataset_sample_ids/same_len_tokenized_sample_ids_gpt2_mrpc.npy"
subset_size: null

# Parameters for the constant noise function
const_noise_params:
  noise_dist: "laplacian" # "laplacian" # "gaussian"
  # The true mean and variance of the noise for the non-axial case
  true_mean: 0.00
  true_variance: 0.055 # 0.05
  initial_mean: 0.00
  initial_variance: 1e0 # 0.01
  # Seed for the random number generator
  seed: 42
  # If you want to load the mean and variance from a file, set this to true
  load_from_file: false
  mean_file: "data/small_dataset_means.npy"
  var_file: "data/small_dataset_vars.npy"
